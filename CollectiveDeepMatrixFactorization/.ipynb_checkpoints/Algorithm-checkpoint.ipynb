{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Modality Disease Modeling via Collective Deep Matrix Factorization\n",
    "(https://dl.acm.org/doi/pdf/10.1145/3097983.3098164)\n",
    "\n",
    "### Abstract\n",
    ">The challenging task of MCI detection is therefore of great clinical importance.\n",
    ">We propose a framework to fuse multiple data modalities for predictive modeling using deep matrix factorization, which explores the non-linear interactions among the modalities and exploits such interactions to transfer knowledge and enable high performance prediction.\n",
    "\n",
    "주요한 점은 2가지 이다. 현재 NL -> MCI -> AD로 가기전에 **MCI를 발견**하는 것이 큰 목표이며, 이러한 MCI발견을 위하여 **여러가지 Modality를 Funsion**하여 MCI를 Prediction을 하겠다는 것 이다.  \n",
    "\n",
    "여러 가지 Modality를 Fusion하기 위하여 Collective Matrix Factorization에서 Non-Linearlity를 증가시키기 위하여 **Collective Deep Matrix Factorization을 사용**하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Completetion\n",
    "Matrix Factorization기법은 Recommend System에서 많이 사용하는 방법 중 하나이다.  \n",
    "\n",
    "먼저 Recommend System에서의 사용방법을 사용하기 위하여 다음과 같은 예제를 생각해보자.  \n",
    "<img src=\"http://latex.codecogs.com/gif.latex?movie.&space;{\\begin{matrix}&space;1&space;&&space;2&space;&&space;3&space;&&space;4&space;&&space;5&space;&6&space;&&space;7&space;&&space;8\\end{matrix}}&space;\\\\&space;\\left\\{\\begin{matrix}&space;user&space;1&space;\\\\&space;user&space;2&space;\\\\&space;user3&space;\\\\&space;user4&space;\\\\&space;user5&space;\\\\&space;user6&space;\\\\&space;user7&space;\\\\user8\\,&space;\\end{matrix}\\right.&space;\\begin{bmatrix}&space;3&space;&&space;5&space;&&space;*&space;&&space;4&space;&&space;1&space;&*&space;&&space;*&space;&&space;2&space;\\\\&space;*&space;&&space;3&space;&&space;5&space;&&space;1&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;3&space;\\\\&space;4&space;&&space;1&space;&&space;*&space;&&space;4&space;&&space;1&space;&*&space;&&space;3&space;&&space;2&space;\\\\&space;5&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;2&space;&&space;3&space;&&space;*&space;&&space;*&space;\\\\&space;*&space;&&space;2&space;&&space;4&space;&&space;2&space;&&space;*&space;&&space;*&space;&&space;1&space;&&space;2&space;\\\\&space;5&space;&&space;*&space;&&space;*&space;&&space;5&space;&&space;4&space;&*&space;&&space;*&space;&&space;4&space;\\\\&space;1&space;&&space;*&space;&&space;5&space;&&space;2&space;&&space;3&space;&1&space;&&space;5&space;&&space;3&space;\\\\&space;*&space;&&space;3&space;&&space;2&space;&&space;1&space;&&space;4&space;&&space;*&space;&&space;*&space;&&space;*&space;\\\\&space;\\end{bmatrix}\"/><br>\n",
    "사진 출처: <a href=\"http://sanghyukchun.github.io/73/\">sanghyukchun 블로그</a><br>\n",
    "\n",
    "위와같이 특정 User에게 Movie를 추천하려고 하나 모든 정보가 없기 때문에 추천할 수 없는 상황이 있다.  \n",
    "\n",
    "이러한 비어있는 Matrix를 채우기 위하여 Matrix Factorization을 사용하게 된다.  \n",
    "\n",
    "이러한 Matrix를 채우는 문제를 **Matrix Completion**이라고 칭하게 된다.\n",
    "\n",
    "### Matrix Factorization\n",
    "**Matrix Completion**을 위하여 사용하는 방법이다.  \n",
    "**Matrix Factorization**의 수식은 매우 간단하다.  \n",
    "<p>$$\\hat{r_{ui}} = p_u \\cdot q_i$$</p>\n",
    "<p>$$min\\text{  }rank(\\hat{R}) \\text{ s.t. }\\Omega(r_{ui}-\\hat{r_ui}) = 0 \\forall u,i$$</p>\n",
    "<p>$$\n",
    "\\Omega(A_{ij}-B_{ij})=\n",
    "\\begin{cases}\n",
    "0, & \\mbox{if }A_{ij} \\mbox{or }B_{ij} = 0 \\\\\n",
    "A_{ij}-B{ij}, & \\mbox{else}\n",
    "\\end{cases}\n",
    "$$</p>\n",
    "\n",
    "\n",
    "위의 수식을 살펴보게 되면 매우 간단하다는 것을 알 수 있다.  \n",
    "실제 가지고 있는 Matrix를 <span>$r_{ui}$</span>라고 가정하면 이 Matrix를 <span>$p_u, q_i$</span>로서 나누게 된다.  \n",
    "\n",
    "즉, <span>$r_{ui} \\in R^{n*m}$</span>이라면, <span>$p_u \\in R^{n*r}, q_i \\in R^{m*r}$</span>, <span>$r << n,m$</span>이라고 생각하자.  \n",
    "\n",
    "**다시 생각해보면, Original Matrix의 Rank는 줄일수 있으며, 이로 인하여 Low Rank Matrix로서 나눌 수 있다. 이러한 Rank를 줄일 수록, Dimension Reduction은 더 가능할 것 이다.**\n",
    "\n",
    "예제로서 생각해보면, 아래와 같을 것 이다.  \n",
    "\n",
    "**user u의 item들에 대한 숨겨진(latent) interest <span>$p_u$</span>와 그에 대응하는 item들의 숨겨진 특성 <span>$q_i$</span>에 의해 결정된다는 사실을 알 수 있다. 이를 그림으로 표현하면 다음과 같다.**\n",
    "<img class=\"center\" src=\"http://sanghyukchun.github.io/images/post/30-1.png\"><br>\n",
    "사진 출처: <a href=\"http://sanghyukchun.github.io/73/\">sanghyukchun 블로그</a><br>\n",
    "\n",
    "### Matrix Factorization Solution\n",
    "다시한번 Matrix Factorization의 식을 살펴보면 다음과 같다.  \n",
    "<p>$$min\\text{  }rank(\\hat{R}) \\text{ s.t. }\\Omega(r_{ui}-\\hat{r_ui}) = 0 \\forall u,i$$</p>\n",
    "\n",
    "**위와 같이 Rank자체를 Minimize하는 Rank Condition은 Non-Convex 문제 이므로 Optimization을 할 수 없다. 따라서 위와 같은 형태를 Convex한 문제로서 변형하는 것은 다음과 같이 2가지 방법이 있다.**  \n",
    "\n",
    "**1. Convex Relaxation**  \n",
    "위의 수식의 Rank를 최소화 하는 문제는 다음과 같이 생각할 수 있다.  \n",
    "<p>$$min \\sum_{l}||\\sigma_{l}(\\hat{R})||_{0} \\text{ s.t.} \\Omega(r_{ui}-\\hat{r_{ui}})=0 \\forall u,i$$</p>\n",
    "\n",
    "**Singular value를 구하고 이가 0이 아닌 값을 Count하는 l0 Norm으로서 나타낼 수 있다는 것 이다.**  \n",
    "\n",
    "Singular Value를 Count하는 문제는 위에서 언급한 대로 Non-Convex한 문제이기 때문에 다음과 같이 식을 변형한다.  \n",
    "\n",
    "<p>$$norm||R||_{*}$$</p>\n",
    "\n",
    "l0 norm -> l1 or l2 .... norm으로서 변형하면서 Convex한 형태로 만드는 것을 **Convex Relaxation**이라고 칭한다.  \n",
    "\n",
    "이러한 방식은 GLobal한 Optimal값을 찾을 수 있지만, 결국 Data에 너무 의존하는 형태가 될 것이다.\n",
    "\n",
    "**2. Solve Non-Convex Problem Directly**  \n",
    "<p>$$min_{\\hat{R}} \\sum_{u,i \\in k}(r_{ui} - \\hat{r_{ui}}) \\text{ s.t }rank(\\hat{R} = k)$$</p>\n",
    "\n",
    "위의 수식을 살펴보게 되면 원래 Matrix Factorization의 Minimize시겡서 s.t.와 Formulation이 변형된 것을 알 수 있다.  \n",
    "\n",
    "**Rank(줄이고자 하는 Dimension)은 고정시키고, 이 Rank에서 s.t를 만족하는 것으로서 Solution을 찾겠다는 의미이다. Global한 Solution값을 찾을 수 없겠지만, Grid Search방식으로서 Local Minimum은 찾을 수 있을 것이고, 이러한 형태는 꽤 성능이 좋다고 알려져 있다.**\n",
    "\n",
    "위와 같은 Matrix Factorization은 Overfitting을 피하기 위하여 다음과 같은 방식으로 Trainning된다.  \n",
    "\n",
    "<p>$$min_{P,Q} \\sum_{u,i \\in k}(r_{ui} - p_{u}\\cdot q_i)^2 + \\lambda(||p_u||_{2}^2+||q_i||_{2}^2)$$</p>\n",
    "\n",
    "### Collective Matrix Factorization\n",
    "위와 같은 Matrix Factorization은 Univariate Analysis인 것을 확인할 수 있을 것 이다.(위에서는 User-Movie로서 1:1 Mapping)  \n",
    "\n",
    "**Multimodality Analysis를 위하여 Collective Matrix Factorization**을 사용하게 된다.  \n",
    "실제 Paper에서의 수식을 살펴보게 되면 다음과 같다.  \n",
    "\n",
    "<p>$$min_{U,V_1,V_2} d(X_1,UV_1^T)+d(X_2,UV_2^T) \\text{ s.t} U \\in S_0, V_i \\in S_{i}, i=1,2$$</p>\n",
    "\n",
    "Matrix Factorization처럼 예시로서 설명하면 다음과 같다.  \n",
    "우리는 n명에 대한 Sample이 있고, 각각 SMRI, SNPs의 Data를 가지고 있다고 생각하면 각각의 Data를 다음과 같이 나타낼 수 있다.\n",
    "- <span>$X_1 \\in R^{r*d_1}$</span>: MRI Data\n",
    "- <span>$X_2 \\in R^{r*d_2}$</span>: SNPs Data\n",
    "\n",
    "Sample의 수는 같을 것이고 Data의 특성에 따라서 Dimension이 달라지게 될 것이다.  \n",
    "이러한 Data를 Matrix Factorization으로서 나누게 되면 다음과 같이 나눠질 수 있다.\n",
    "- <span>$X_1 = UV_1 \\text{ s.t }U \\in R^{n*r}, V_1 \\in R^{r*d_1}$</span>\n",
    "- <span>$X_2 = UV_2 \\text{ s.t }U \\in R^{n*r}, V_2 \\in R^{r*d_2}$</span>\n",
    "\n",
    "즉, 각각의 Matrix Factorization에서의 각각의 Sample에 대한 특성은 공유하면서, Data에 따라서 특성이 다르게 Matrix Factorization이 가능하다는 것 이다.  \n",
    "\n",
    "공통적인 특성인 U를 공유함으로서 Multi Modality Analysis가 가능하다는 것 이다.\n",
    "\n",
    "### Collective Deep Matrix Factorization\n",
    "![png](./image/1.png)\n",
    "\n",
    "위의 Figure는 현재 Paper에서 설명하는 Collective Deep Matrix Factorization에 대한 내용이다.  \n",
    "\n",
    "Collective Matrix Factorization에서 Non-Linearity를 증가시키기 위하여 ANN을 사용했다는 것 이다.  \n",
    "수식으로서 살펴보면 다음과 같다.  \n",
    "\n",
    "<p>$$min_{U, (V_i, \\theta_i)_{i=1}^{t}} \\sum_{i=1}^{t} d(X_i,U g_{\\theta_i}(V_i)) \\text{ s.t} U \\in S_0, V_i \\in S_i$$</p>\n",
    "<p>$$g_{\\theta_i}(V_i) = f(W_{(k,i)}f(W_{(k-1,i)}f(...,W_{(1,i)}V_i))$$</p>\n",
    "\n",
    "위의 식을 Collective Matrix Factorization과 비교하면 <span>$V \\rightarrow g_{\\theta_i}(V_i)$</span>로서 ANN의 결과로서 사용하는 것을 살펴볼 수 있다.  \n",
    "**즉, Collective Deep Matrix Factorization의 핵심은 다음과 같다.**\n",
    "1. U를 공유함으로서 Multi-Modality Analysis가 가능하다.\n",
    "2. U를 통하여 Prediction하는 경우에 Dimension Reduction이 된다.\n",
    "3. ANN을 사용하여 단순한 Matrix Multiply가 아닌 Matrix * ANN으로서 Non-Linearity를 증가시켰다\n",
    "\n",
    "실제 Trainning과 각각의 Modality의 Significant를 측정하기 위하여 최종적인 식은 다음과 같이 정의하였다.  \n",
    "\n",
    "<p>$$min_{U, (V_i, \\theta_i)_{i=1}^{t}} \\sum_{j=1}^{n} l(h(U_j;w),y_j) +  \\sum_{i=1}^{t} \\alpha_i d(X_i,U g_{\\theta_i}(V_i)) \\text{ s.t} U \\in S_0, V_i \\in S_i, \\forall_i$$</p>\n",
    "<p>$$l(h(U_j;w),y_j): \\text{ Logistic Regression}$$</p>\n",
    "<p>$$g_{\\theta_i}: \\text{ Same architecture and share the same parameter values, except for the last layer}$$</p>\n",
    "\n",
    "최종적인 목적은 Dataset에 대하여 NL or MCI로서 판단하는 것 이다.  \n",
    "식을 살펴보게 되면 크게 2개의 Term으로서 이루워질 수 있는 것을 볼 수 있고 각각은 다음과 같은 의미를 가지고 있다.\n",
    "- 1 Term: <span>$l(h(U_j;w),y_j)$</span>: U(Latent Representation)으로서 NL or MCI를 Prediction\n",
    "- 2 Term: <span>$\\alpha_i d(X_i,U g_{\\theta_i}(V_i))$</span>: U를 Update하기 위하여 사용한다. <span>$\\alpha_i$</span>를 통하여 각각의 Modlaity의 중요도를 결정하게 된다. **만약 <span>$\\alpha_i$</span>가 크다면, 그 Modality에 대하여 같은 Epoch에 대하여 더 Trainning을 잘 보게 될 것이다. 즉, <span>$\\alpha_i$</span>가 클수록 Modality에 대한 중요도를 높인 것이라고 생각할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "**Initialization**  \n",
    "현재 논문에서는 다음과 같이 표현하고 있다.  \n",
    ">Since the objective in highly non-convex and gradient algorithms may easily trapped in local optima, a good initialization is important for training the network => Initialization: SVD\n",
    "\n",
    "위에서 적은 Matrix Factorization의 문제를 살펴보게 되면, 다음과 같은 문제가 있는 것을 확인할 수 있었다.\n",
    "1. Matrix Factorization의 Cost Funciton은 Non-Convex이므로 Optimal한 값을 찾을 수 없다.\n",
    "2. Convex Relaxation으로서 Optimal Solution을 찾을 수 있지만, Data에 너무 의존하게 되어서, 식이 성립안할 수 있다.\n",
    "3. 위와 같은 문제로 인하여 Non-convex Problem으로서 Directly로 구하게 되면, Rank자체를 고정하고, 그에 따른 Local Optimum을 찾는 문제로서 해결한다.\n",
    "\n",
    "이러한 Local Optimum에 빠지게 되는 것을 최대한 방지하고자 하는 방법이 Initialization으로서 SVD를 사용하는 것이다.  \n",
    "현재 Paper에서도 Initialization을 SVD를 사용하여 성능을 향상시켰다고 한다.  \n",
    "\n",
    "Initialization을 SVD로서 사용하는 이유는 논문을 참조하자  \n",
    "논문: SVD based initialization: A head start for nonnegative matrix factorization  \n",
    "(https://www.sciencedirect.com/science/article/pii/S0031320307004359)\n",
    "\n",
    "**Imaging modalities preprocessing**  \n",
    "현재 논문에서는 ADNI1, ADNI2에 대하여 사용하였다.  \n",
    "또한 논문에서 강조하고 있는 것이 Image를 Preprocessing하는 과정에서 Age와 Sex, 그리고 각각의 Data 특성을 고려해야 한다는 것 이다.  \n",
    "\n",
    "해당 Formulation은 다음과 같다.  \n",
    "<p>$$X^{obs} = w_1 \\cdot age + w_2 \\cdot sex + w_3 \\cdot cohort + X^{ori}$$</p>\n",
    "\n",
    "- <span>$X^{obs}$</span>: 실제 값\n",
    "- <span>$X^{ori}$</span>: age, sex, cohort와 관계가 없는 값(사용하고자 하는 값)\n",
    "- <span>$cohort$</span>: ADNI1: 1, ADNI2: -1\n",
    "\n",
    "실제Training은 다음과 같이 진행된다.\n",
    "<p>$$w^{*} = min_w \\sum_{i=1}^{n} (x^t t_i-X_i^{obs})^2$$</p>\n",
    "따라서 위와 같이 실제 Data인 <span>$X^{obs}$</span>에 대하여 age, sex, cohort로서 표현 가능하면 이러한 영향이 없는 <span>$X^{ori}$</span>는 다음과 같이 적을 수 있다.\n",
    "\n",
    "<p>$$X^{ori} = X^{obs} - (w_1^{*}\\cdot age+w_2^{*}\\cdot sex+w_3^{*}\\cdot cohort)$$</p>\n",
    "\n",
    "Age correction등이 MRI Image을 통하여 Classification에서 성능이 향상된 예시는 해당 논문은 다음과 같다.  \n",
    "논문: The Effect of Age Correction on Multivariate Classification in Alzheimer’s Disease, with a Focus on the Characteristics of Incorrectly and Correctly Classified Subjects  \n",
    "(https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4754326/)\n",
    "\n",
    "위의 논문에서 Age Correction Method부분을 살펴보면 다음과 같이 이야기 하고 있다.\n",
    ">**The detrending algorithm fits a generalized linear model (GLM) to each MRI-derived variable and age, in the CTL group only, and models the age-related changes as a linear drift. Then, the regression coefficient of the resulted GLM model (linear drift) is used to remove the age-related changes from all individuals (AD, MCI and CTL) and obtain corrected values. The linear model was chosen based on the Good et al. (2001) study where they found an age-related linear decrease in global grey matter volume in healthy individuals.**\n",
    "\n",
    "GrayMatter는 Age가 많아짐에 따라서 Linear Decrease가 있기 때문에, 이에 따른 Correction을 하여 Dataset에서 Age에 대한 Effect가 없어지도록 Preprocessing하는 것이 Model의 성능을 향상시킨다는 것 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "Result의 경우에는 크게 2가지로 나누어서 생각할 수 있다.\n",
    "\n",
    "**1. Predict Performance**  \n",
    "실제 MRI or NL을 Prediction하는 Model의 성능에 대하여 해당 논문은 다음과 같은 Performance를 보여주었다고 한다.  \n",
    "\n",
    "Prediction performance of different models using **ADNI2's T1 MRI and dMRI in terms of AUC**. With an appropriate activation function and components' number, our method outperfoms than all other methods.  \n",
    "![png](./image/2.png)\n",
    "\n",
    "Prediction performance of different models using **ADNI2's and ADNI1's T1 MRI and dMRI in terms of AUC**. With an appropriate activation function and components' number, our method outperfoms than all other methods.  \n",
    "![png](./image/3.png)\n",
    "\n",
    "**Prediction performance of fusing genetic knowledge and imaging knowledge using ADNI1 and ADNI2 in terms of AUC. Genetic modality can be successfully integrated with imaging modalities.**  \n",
    "![png](./image/4.png)\n",
    "\n",
    "Components란 <span>$g_{\\theta_i}(V_i)$</span>의 2번째 Layer를 의미하게 되고, 각각의 Activation Function과 Model을 바꿔가면서 측정한 결과이다.\n",
    "\n",
    "**중요하게 봤던 점은 Genetic Data를 추가함으로서 AUC가 증가한다는 것 이다. 즉, Image Data와 Genetic Data를 활용하여 Multivariate Analysis를 휼륭하게 수행하였다는 점 이다.**  \n",
    "\n",
    "**2. Effects of knowledge fusion parameters**  \n",
    "![png](./image/5.png)\n",
    "\n",
    "- <span>$\\alpha_1$</span>: dMRI\n",
    "- <span>$\\alpha_2$</span>: T1\n",
    "- <span>$\\alpha_3$</span>: SNPs\n",
    "\n",
    "위의 LossFunction을 다시 살펴보면 다음과 같다.\n",
    "<p>$$min_{U, (V_i, \\theta_i)_{i=1}^{t}} \\sum_{j=1}^{n} l(h(U_j;w),y_j) +  \\sum_{i=1}^{t} \\alpha_i d(X_i,U g_{\\theta_i}(V_i)) \\text{ s.t} U \\in S_0, V_i \\in S_i, \\forall_i$$</p>\n",
    "\n",
    "위의 식에서 <span>$\\alpha_i$</span>를 변형시켜가면서 얻은 Model의 AUC에 대한 Graph이다.  \n",
    "**<span>$\\alpha_1, \\alpha_2$</span>는 같은 Image Data로서 값이 비슷하게 변하는 결과를 보여주었다면, <span>$\\alpha_3$</span>의 경우는 0.1을 넘는순간 급격하게 AUC가 감소되는 것을 확인할 수 있다.**  \n",
    "\n",
    "이러한 결과에 대하여 해당 Paper는 다음과 같이 얘기하고 있다.\n",
    ">That is because genetic modality is noiser than imaging modalityies. With a small a3, , this model can tolerant a lager reconstruction error for genetic modality. Hence, the model is robust to the noise in genetic modality.\n",
    "\n",
    "Genetic Data가 Image Data보다 Noise가 많다는 것 이고, 이에 따라서 <span>$\\alpha_3$</span>의 값을 작게하여서, Noise에 대하여 Robust한 Model을 구축하였다고 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
